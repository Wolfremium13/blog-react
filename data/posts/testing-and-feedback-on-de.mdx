---
title: "üß™ Ciclos de feedback y Testing en DE"
date: "2022-12-07"
preview: "/assets/blog/testing-and-feedback-on-de/sci-cat.webp"
tags:
  - "üë®‚Äçüíª Aprendizaje"
  - "üß† Reflexi√≥n"
  - "üóû Data Engineering"
---

## ¬øPor qu√© me interesa reducir el ciclo de feedback?

Despu√©s de algunos meses trabajando con datos me he dado cuenta la importancia
de estos ciclos, desde el punto de vista de negocio es fundamental poder
generar/corregir/actualizar datos en el menor tiempo posible para poder tomar
acciones. Para m√≠ el ciclo no solo depende de cuanto tarde el c√≥digo en
transformar datos y dejarlos en la aplicaci√≥n, la mantenibilidad del c√≥digo y
sus tests afectan directamente a estos ciclos, reduciendo el n√∫mero de bugs con
los test o validaciones apropiadas.

En pocas palabras te digo que el valor aportado a negocio crece un mont√≥n cuando
sabemos invertir la pir√°mide de los test invertida, b√°sicamente reduciendo el
n√∫mero de tests de integraci√≥n.

<MdxImage
  src="/assets/blog/testing-and-feedback-on-de/inverted-test-pyramid.webp"
  alt="inverted-test-pyramid"
></MdxImage>

El tiempo es algo superimportante tanto en la vida como en el trabajo, ¬øno ser√≠a
maravilloso poder salir antes debido a un buen trabajo realizado?

## Tama√±o de los ciclos de feedback y estrategias

Pues ver√°s, me he topado con este dilema muchas veces, en mi caso probar algo de
principio a fin en ‚Äúproducci√≥n‚Äù significa esperar 2 horas a que los c√°lculos
terminen. Al principio intent√© reducirlo con test unitarios que comprobasen cada
aspecto del proceso de c√°lculos, veamos un ejemplo:

```python
import pandas as pd

def test_some_behavior_of_your_transformation(self):
    stub = read_from_a_json_file()
    expected = read_from_a_json_file()

    result = self.calculator.do_some_stuff(stub)

    pd.testing.assert_frame_equal(result, expected)
```

Una mala idea, en este caso el c√≥digo de test sabe lo mismo o incluso m√°s que el
c√≥digo que se ocupa de transformar, lo cual lo vuelve muy fr√°gil al cambio, por
no decir que construir los datos se hace infernal a veces. Si muchas filas en tu
DataFrame se vuelve poco legible, si falla algo usando las aserciones de pandas.
Entonces la soluci√≥n fue simplificar al m√°ximo lo qu√© realmente quer√≠amos
testear usando aserciones creadas por m√≠ o directamente `assert_that` de
`AssertPy`.

```python
from assertpy import assert_that

def test_some_behavior_of_your_transformation(self):
    stub = generate_data_using_a_builder(value=True)

    result = self.calculator.do_some_stuff(stub)

    assert_that(result).not_contains(True)
```

As√≠, reduciendo la fragilidad y mejorando la mantenibilidad de los tests, se
reduce en gran medida el tiempo feedback, estar seguro de tu c√≥digo es una
herramienta no solo importante para no perder el contexto de lo que estabas
intentando implementar, sino para no perder el tiempo andando por la senda m√°s
larga. No es una sorpresa que te diga que por mucho esfuerzo que pongas en tests
unitarios no siempre es suficiente, un ejemplo:

Nuevos datos ‚Üí Nuestro proceso de transformaci√≥n ‚Üí Validaciones funcionales ‚Üí
Ingesta ‚Üí Validaciones manuales sobre la aplicaci√≥n

En este ejemplo vemos un poco el flujo qu√© los datos recorren normalmente, c√≥mo
puedes ver el ciclo de feedback de principio a fin es enorme, por lo que
introducir un error en los `Nuevos datos` a veces se puede capturar a mitad del
proceso en la propia transformaci√≥n, ya que tu c√≥digo no acepta valores que no
deber√≠an estar ah√≠ por lo que en cierta manera hemos ahorrado tiempo/esfuerzo a
los siguientes pasos. No siempre esto es as√≠ y hay veces que por cosas de la
vida es imposible de atrapar casos de uso nuevo u errores sin tenerlos en
cuenta.

T√©cnicas como Property Based Testing o Exploratory Testing nos pueden ayudar a
intentar reducir el ciclo de feedback a cambio de aumentar tiempos en el ciclo
de feedback m√°s peque√±o, buscar el punto correcto de retorno de inversi√≥n corre
a nuestra cuenta.

## Duplicidad y solapamiento de los tests

Es muy importante reducir lo m√°ximo posible la duplicidad del contenido a
testear, por ejemplo, si al guardar un data frame de pandas le pasamos un
esquema de pyarrow para forzar los tipos a cada columna y chequear si esta no es
nula, no tenemos porque nosotros tener un test que lo compruebe. En vez de eso
podemos generar un wrapper de la llamada a la funci√≥n que fuerce pasar un
esquema y validando su llamada con un test a m√°s alto nivel.

Al principio cuesta verlo, pero tener tests que realmente tengan √°mbitos
diferentes y que solo fallen por una cosa marca la diferencia, es muy raro que
tu test de m√°s alto nivel sepa exactamente que devuelve cada elemento de
transformaci√≥n. Suele darse este caso en c√≥digo que he visto muy frecuentemente:

```python
class Transformer:
    # All the transformation methods are in this file.

    df2 = add_some_columns(df1, config)
    df1 = tranform_some_columns(df1)
    df2 = merge_some_data(df1, df2)
    # Random redudant comment
    clean_df = drop_duplicates(df2)

    # Imagine more tranformation code and save
```

Por ello he optado en agrupar y dejar la menor l√≥gica posible en la capa m√°s
alta del c√≥digo, simplemente su orden, que es lo que realmente nos importa en
este nivel. Por tanto, si nos es imposible testear esa clase debido a multitud
de dependencias o configuraciones, no habr√° tanto problema, ya que estamos
probando cada objeto de transformaci√≥n por separado, evitando as√≠ complejidad,
intent√°ndolos llamar y mirar al detalle desde la capa m√°s alta de nuestra
transformaci√≥n.

```python
class Transformer:
    # All the transformation methods are in separated files.
    # And we test each function isolated.

    df2 = prepare_data(config)

    result = (
        df1
        .pipe(prepare_data)
        .pipe(add_some_columns, config)
        .pipe(tranform_some_columns)
        .pipe(merge_some_data, df2)
        .pipe(clean_results)
        # etc.
    ).to_csv(dst, index=False)
```

## Conclusi√≥n

Los conceptos de testing son transferibles a cualquier rama de la programaci√≥n,
el hecho de poner `TDD on <tu-especialidad>` no marca la una diferencia notable
en las pr√°cticas empleadas o las soluciones propuestas, nada de lo dicho aqu√≠ es
nuevo, pero para m√≠ si lo es darme cuenta.
